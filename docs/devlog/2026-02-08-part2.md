# Devlog â€” 2026-02-08 (Part 2)

## Milestone 8: Sidecar comparison toolkit (memory-core adapter + dual-language pilot)

Completed:
- Added `memory-core` adapter (`src/openclaw_memory_bench/adapters/memory_core.py`):
  - Isolation via `openclaw --profile membench-memory-core`
  - Automatic profile memory clear per benchmark question
  - Session markdown generation (`session-<id>.md`)
  - CLI-driven workflow: `openclaw memory index --force` + `openclaw memory search --json`
- Added dual-language mini dataset (`examples/dual_language_mini.json`):
  - 4 questions (zh-zh / en-en / zh-en / en-zh cross-lingual retrieval)
  - Use-case aligned: CK's real dual-language agent workflow scenarios
- Added sidecar compare orchestrator:
  - `scripts/run_memory_core_vs_openclaw_mem.py`
  - `scripts/run_memory_core_vs_openclaw_mem.sh`
  - Runs both `memory-core` and `openclaw-mem` on identical dataset
  - Emits delta metrics (openclaw-mem - memory-core)
  - Artifacts: `compare-<run-group>.json` + `compare-<run-group>.md`
- Updated CLI and tests:
  - `--provider memory-core` support + associated flags (`--memory-core-profile`, `--memory-core-agent`, `--memory-core-timeout-sec`)
  - Updated `tests/test_smoke.py` (assert `memory-core` in adapters)
  - Tests passing: 9 passed
- Updated README with memory-core usage examples and sidecar comparison workflow.

## First sidecar pilot run (20260208T233812Z-ck-sidecar-pilot)

Dataset: `examples/dual_language_mini.json` (4 questions, top_k=5)

### Results summary

**memory-core (OpenClaw built-in)**:
- Hit@5: **1.00** (4/4 questions retrieved relevant session in top-5)
- Recall@5: **1.00**
- MRR: **1.00**
- nDCG@5: **1.00**
- Latency p50/p95: **17.6s / 18.2s** (extremely slow, likely due to index rebuild overhead per question)

**openclaw-mem (sidecar path)**:
- Hit@5: **0.75** (3/4 questions hit)
- Recall@5: **0.75**
- MRR: **0.625**
- nDCG@5: **0.658**
- Latency p50/p95: **145ms / 175ms** (~120x faster)

### Key findings

1. **Retrieval quality gap**:
   - openclaw-mem missed 1 out of 4 questions (likely due to FTS tokenization or query sanitization difference).
   - memory-core achieved perfect recall on this small dual-language dataset.

2. **Latency tradeoff**:
   - openclaw-mem is **~120x faster** (145ms vs 17.6s p50).
   - memory-core's slowness is likely due to full index rebuild + vector embedding overhead on each question clear/ingest cycle (not representative of production steady-state performance).

3. **Isolation verification**:
   - Both adapters ran in isolated environments (no main OpenClaw pollution).
   - memory-core: `--profile membench-memory-core` (independent workspace/state).
   - openclaw-mem: per-run DB root (`artifacts/provider-state/openclaw-mem`).

### Next steps

- Investigate openclaw-mem FTS query sanitization for the missed question (likely related to cross-lingual or hyphenated token handling).
- Add memory-lancedb adapter for 4-way comparison (memory-core / memory-core+sidecar / memory-lancedb / memory-lancedb+sidecar).
- Expand dual-language dataset to 20+ questions for more robust signal.
- Run larger benchmark (LongMemEval subset) to validate retrieval quality at scale.

Artifacts: `artifacts/sidecar-compare/20260208T233812Z-ck-sidecar-pilot/`
