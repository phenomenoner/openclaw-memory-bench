# Devlog â€” 2026-02-08

## Milestone 1: Repository bootstrap

Completed:
- Initialized standalone benchmark toolkit scaffold.
- Added MIT license.
- Added acknowledgements policy for `supermemoryai/memorybench`.
- Added minimal Python 3.13 + uv project setup.
- Added CLI skeleton:
  - `doctor` (environment + adapter inventory)
  - `plan` (run-manifest generator)
- Added adapter protocol and two adapter entries:
  - `openclaw-mem` (initial CLI-based ingest/search adapter)
  - `memu-engine` (stub for next implementation step)
- Added ADR 0001 documenting scope/license decision.

## Milestone 2: Early planning docs

Completed:
- Added draft benchmark spec (`docs/spec-v0.1.md`).
- Added `memu-engine` adapter implementation plan (`docs/memu-adapter-plan.md`).

## Milestone 3: Retrieval-track executable MVP

Completed:
- Added retrieval dataset loader (`src/openclaw_memory_bench/dataset.py`).
- Added deterministic retrieval metric engine (`src/openclaw_memory_bench/metrics.py`).
- Added retrieval benchmark runner + report writer (`src/openclaw_memory_bench/runner.py`).
- Added executable CLI command: `run-retrieval`.
- Implemented `openclaw-mem` adapter isolation strategy (per-container SQLite DB under `--db-root`).
- Added example dataset (`examples/mini_retrieval.json`) and dataset format docs.
- Added unit tests for dataset parsing and retrieval metrics.
- Verified executable smoke run with `openclaw-mem` on `examples/mini_retrieval.json` (non-interactive CLI path).

Next:
1. Add dataset adapters/importers for LoCoMo / LongMemEval / ConvoMem canonical splits.
2. Implement `memu-engine` adapter via Gateway tool-invoke route.
3. Add run manifest embedding into final reports for stronger reproducibility.
4. Add baseline CI workflow (smoke retrieval run + artifact upload).
