# Devlog â€” 2026-02-08

## Milestone 1: Repository bootstrap

Completed:
- Initialized standalone benchmark toolkit scaffold.
- Added MIT license.
- Added acknowledgements policy for `supermemoryai/memorybench`.
- Added minimal Python 3.13 + uv project setup.
- Added CLI skeleton:
  - `doctor` (environment + adapter inventory)
  - `plan` (run-manifest generator)
- Added adapter protocol and two adapter entries:
  - `openclaw-mem` (initial CLI-based ingest/search adapter)
  - `memu-engine` (stub for next implementation step)
- Added ADR 0001 documenting scope/license decision.

## Milestone 2: Early planning docs

Completed:
- Added draft benchmark spec (`docs/spec-v0.1.md`).
- Added `memu-engine` adapter implementation plan (`docs/memu-adapter-plan.md`).

## Milestone 3: Retrieval-track executable MVP

Completed:
- Added retrieval dataset loader (`src/openclaw_memory_bench/dataset.py`).
- Added deterministic retrieval metric engine (`src/openclaw_memory_bench/metrics.py`).
- Added retrieval benchmark runner + report writer (`src/openclaw_memory_bench/runner.py`).
- Added executable CLI command: `run-retrieval`.
- Implemented `openclaw-mem` adapter isolation strategy (per-container SQLite DB under `--db-root`).
- Added example dataset (`examples/mini_retrieval.json`) and dataset format docs.
- Added unit tests for dataset parsing and retrieval metrics.
- Verified executable smoke run with `openclaw-mem` on `examples/mini_retrieval.json` (non-interactive CLI path).

## Milestone 4: Next-stage baseline expansion

Completed:
- Added canonical dataset conversion pipeline (`prepare-dataset`) for:
  - LoCoMo
  - LongMemEval
  - ConvoMem
- Implemented `memu-engine` adapter via Gateway `tools/invoke` (`memory_search`, optional `memory_store` ingest mode).
- Added reusable Gateway client utility with local config/env fallback.
- Added CI workflow (`.github/workflows/ci.yml`) for lint + tests + retrieval smoke run + artifact upload.
- Added `--skip-ingest` run mode for search-only benchmarking against pre-ingested providers.

## Milestone 5: Pause-ready planning docs

Completed:
- Added consolidated project roadmap + TODO tracker: `docs/PROJECT_PLAN_AND_TODOS.md`.
- Linked roadmap doc from `README.md` for easy resume path.

## Milestone 6: Reproducibility hardening + CI fix

Completed:
- Fixed CI workflow YAML indentation issues (`uses`/`with`) that caused GitHub Actions parse-time failure.
- Added artifact upload guard (`if-no-files-found: ignore`) for environments without retrieval artifacts.
- Implemented retrieval run-manifest embedding in reports (`retrieval-report/v0.2`):
  - toolkit version
  - git commit
  - dataset path + SHA256 + sidecar conversion metadata
  - provider config (token/secret fields redacted)
  - runtime metadata (python/platform/executable)
- Added dataset sidecar metadata output from `prepare-dataset` (`*.meta.json`) with benchmark source URLs.
- Added manifest unit tests.

## Milestone 7: Phase A orchestration baseline

Completed:
- Added locked two-plugin run profile: `configs/run-profiles/two-plugin-baseline.json` (LongMemEval-100, top_k=10).
- Added one-shot orchestrator:
  - `scripts/run_two_plugin_baseline.py`
  - `scripts/run_two_plugin_baseline.sh`
- Implemented automatic compare artifact generation (`compare-<run-group>.json/.md`).
- Implemented memu fallback policy in orchestrator:
  - try `memory_store` ingest first
  - fallback to `noop + --skip-ingest` and record reason
- Added execution guide: `docs/PHASE_A_EXECUTION.md`.

Next:
1. Add benchmark schema validation + richer error classification.
2. Add provider capability matrix and per-provider recommended run recipes.
3. Run official LongMemEval-100 two-plugin baseline and publish frozen report artifacts.
